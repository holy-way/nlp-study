Studying From https://wikidocs.net/217238

# Cleaning and Normalization 

* Cleaning : Cleaning noise data from corpus
* Normalization : Unify various forms of word into same word

Cleaning is conducted before and after tokenization.   
It can be done through the projects.

# 1. Unifying words based on rules - (normalization)
ex)
* USA = US
* uh-huh = uhhuh

# 2. Unifying upper and lower case - (normalization)

