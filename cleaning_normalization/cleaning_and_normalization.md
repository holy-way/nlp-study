Studying From https://wikidocs.net/217238

# Cleaning and Normalization 

* Cleaning : Cleaning noise data from corpus
* Normalization : Unify various forms of word into same word

Cleaning is conducted before and after tokenization.   
It can be done through the projects.

# 1. (normalization) Unifying words based on rules
ex)
* USA = US
* uh-huh = uhhuh

# 2. (normalization) Unifying upper and lower case
